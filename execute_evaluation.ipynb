{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Required information\n",
    "#### Make sure to fill this cell to run the file"
   ],
   "id": "aa59af49f5751077"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "\n",
    "# Pinecone API key as a string\n",
    "PINECONE_API_KEY = \"\"\n",
    "\n",
    "# Cohere API key as a string\n",
    "COHERE_API_KEY = \"\"\n",
    "\n",
    "# Directory where all the dataset that will be created during the run of this file will be save\n",
    "datasets_saving_dir = \"\"\n",
    "\n",
    "# Directory to the ground-truth dataset provided in the GitHub repository\n",
    "ground_truth_df_dir = \"\"\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ],
   "id": "9a61b6f7be5b9091"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Loading the Neural Bridge RAG Dataset",
   "id": "3c94d7143063842"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "splits = {'train': 'data/train-00000-of-00001-9df3a936e1f63191.parquet', 'test': 'data/test-00000-of-00001-af2a9f454ad1b8a3.parquet'}\n",
    "df = pd.read_parquet(\"hf://datasets/neural-bridge/rag-dataset-12000/\" + splits[\"train\"])\n",
    "df['words_count'] = df['context'].apply(lambda text: len(text.split()))\n",
    "df = df.reset_index()\n",
    "df['ground_truth'] = \"\"\n",
    "df.head()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Basic info and stats of the dataset",
   "id": "d4256fbfeb89fac7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "min_length = min(df['words_count'])\n",
    "max_length = max(df['words_count'])\n",
    "avg_length = df['words_count'].mean()\n",
    "med_length = df['words_count'].median()\n",
    "print(f'min_length: {min_length}')\n",
    "print(f'max_length: {max_length}')\n",
    "print(f'avg_length: {avg_length}')\n",
    "print(f'med_length: {med_length}')"
   ],
   "id": "346e1801fff48746"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Filtering all rows that have texts longer than 550 tokens, and printing info and stats",
   "id": "8ca92ce483ad108e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "large_df = df[df['words_count'] > 550]\n",
    "\n",
    "# Basic info about the df with large documents\n",
    "min_length = min(large_df['words_count'])\n",
    "max_length = max(large_df['words_count'])\n",
    "avg_length = large_df['words_count'].mean()\n",
    "med_length = large_df['words_count'].median()\n",
    "print(f'min_length: {min_length}')\n",
    "print(f'max_length: {max_length}')\n",
    "print(f'avg_length: {avg_length}')\n",
    "print(f'med_length: {med_length}')"
   ],
   "id": "d77b10f688d36c6a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Now we will create the Documents objects from the dataframe, that is needed for the LangChain chunking methods and vectordb",
   "id": "bf8902fac944fd8c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create Document objects list from the df\n",
    "from langchain.schema import Document\n",
    "documents = []\n",
    "for _, row in large_df.iterrows():\n",
    "    doc = Document(\n",
    "        page_content=row['context'],\n",
    "        metadata={'idx': row['index'], 'num_words': row['words_count']},\n",
    "    )\n",
    "    documents.append(doc)\n",
    "\n",
    "q_and_a_df = large_df[['index', 'question', 'answer']]"
   ],
   "id": "d11c3c3d26d7678f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Measuring chunking time and size",
   "id": "911b24008d9a85e5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Init a RecursiveChunker\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "embeddings_model_name = 'sentence-transformers/all-MiniLM-L6-v2' # 512 tokens as input\n",
    "embeddings_model_tokenizer = AutoTokenizer.from_pretrained(embeddings_model_name)\n",
    "\n",
    "rec_text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    "    separators=['\\n\\n','\\n','.','?','!',';',',',' ',''],\n",
    "    length_function=lambda text: len(embeddings_model_tokenizer.encode(text=text,truncation=False))\n",
    ")"
   ],
   "id": "4fca71e028bba0e0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Init a SemanticChunker\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "embeddings_model = HuggingFaceEmbeddings(model_name=embeddings_model_name, model_kwargs={'device': device})\n",
    "\n",
    "sem_text_splitter = SemanticChunker(embeddings=embeddings_model)"
   ],
   "id": "7459177d4ab55be8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Init a SemanticGraphChunker\n",
    "from semantic_graph_chunker import SemanticGraphChunker\n",
    "sem_g_text_splitter = SemanticGraphChunker()"
   ],
   "id": "e15906f77137d293"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Defining a function to measure chunking times and sizes for the chunking methods",
   "id": "8959dbe23c141d33"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Function to measure chunking times\n",
    "\n",
    "def measure_chunking_time_and_num_of_chunks(documents, text_splitter, intervals):\n",
    "    times = []\n",
    "    num_chunks = []\n",
    "    total_time = 0\n",
    "    total_num_of_chunks = 0\n",
    "    prev_interval = 0\n",
    "    all_chunks = []\n",
    "\n",
    "    for interval in tqdm(intervals, desc=\"Processing intervals\"):\n",
    "        # Only process new documents from the last interval to the current one\n",
    "        docs = documents[prev_interval:interval]\n",
    "        start_time = time.time()\n",
    "        chunks = text_splitter.split_documents(docs)\n",
    "        end_time = time.time()\n",
    "\n",
    "        # Update cumulative metrics\n",
    "        chunk_time = end_time - start_time\n",
    "        total_time += chunk_time\n",
    "        total_num_of_chunks += len(chunks)\n",
    "\n",
    "        # Store results\n",
    "        times.append(total_time)\n",
    "        num_chunks.append(total_num_of_chunks)\n",
    "\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            chunk.metadata['chunk_index'] = prev_interval + i\n",
    "            all_chunks.append(chunk)\n",
    "\n",
    "        # Update previous interval\n",
    "        prev_interval = interval\n",
    "\n",
    "    print('Done chunking for all intervals')\n",
    "    return times, num_chunks, all_chunks"
   ],
   "id": "3dd590149a994cce"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Documents intervals\n",
    "intervals = [10,50,100,500,1000,1500,2000,2500,3000,3500,4000,4500,len(documents)]\n"
   ],
   "id": "ccd7786d29a9fbef"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Executing the function for each of the chunking methods",
   "id": "ec7d569ff7d97170"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Measuring times for each chunking method\n",
    "rec_times, rec_num_chunks, rec_chunks = measure_chunking_time_and_num_of_chunks(documents=documents,text_splitter=rec_text_splitter, intervals=intervals)\n",
    "\n",
    "sem_g_times, sem_g_num_chunks, sem_g_chunks = measure_chunking_time_and_num_of_chunks(documents=documents,text_splitter=sem_g_text_splitter, intervals=intervals)\n",
    "\n",
    "sem_times, sem_num_chunks, sem_chunks = measure_chunking_time_and_num_of_chunks(documents=documents,text_splitter=sem_text_splitter, intervals=intervals)"
   ],
   "id": "3a976f3305696100"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Plotting results",
   "id": "3e1275f6fb768364"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Plotting results for time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(intervals, rec_times, label=\"RecursiveChunker\")\n",
    "plt.plot(intervals, sem_times, label=\"SemanticChunker\")\n",
    "plt.plot(intervals, sem_g_times, label=\"SemanticGraphChunker\")\n",
    "\n",
    "# Add labels, title, and legend\n",
    "plt.xlabel(\"Number of Documents\")\n",
    "plt.ylabel(\"Time (seconds)\")\n",
    "plt.title(\"Chunking Time Comparisons\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "# Display the plot\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "id": "7bdb1dd81be04f02"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Plotting results for number of chunks\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(intervals, rec_num_chunks, label=\"RecursiveChunker\")\n",
    "plt.plot(intervals, sem_num_chunks, label=\"SemanticChunker\")\n",
    "plt.plot(intervals, sem_g_num_chunks, label=\"SemanticGraphChunker\")\n",
    "\n",
    "# Add labels, title, and legend\n",
    "plt.xlabel(\"Number of Documents\")\n",
    "plt.ylabel(\"Number of Chunks\")\n",
    "plt.title(\"Comparing number of Chunks\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "# Display the plot\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "id": "7896a1686d71cfe9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Setting up vectorstores for each chunking methods\n",
   "id": "84240d63a2b500f3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Setting up vectorstores\n",
    "\n",
    "\n",
    "def create_vectorstore(documents, index_name):\n",
    "    embedding_model = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2', model_kwargs={'device': device})\n",
    "    pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "    if index_name not in [index_info[\"name\"] for index_info in pc.list_indexes()]:\n",
    "        pc.create_index(\n",
    "            name=index_name,\n",
    "            dimension=len(embedding_model.embed_query('dummy')),\n",
    "            metric=\"cosine\",\n",
    "            spec=ServerlessSpec(cloud=\"aws\", region='us-east-1')\n",
    "        )\n",
    "        while not pc.describe_index(index_name).status[\"ready\"]:\n",
    "            time.sleep(1)\n",
    "\n",
    "    index = pc.Index(index_name)\n",
    "    vectorstore = PineconeVectorStore(index=index, embedding=embedding_model)\n",
    "\n",
    "    # Multiprocessing for faster document addition\n",
    "    batch_size = 100\n",
    "    num_batches = len(documents) // batch_size + (len(documents) % batch_size > 0)\n",
    "\n",
    "    # Add tqdm to track progress\n",
    "    for i in tqdm(range(num_batches), desc=\"Adding documents to Pinecone\"):\n",
    "        batch = documents[i * batch_size:(i + 1) * batch_size]\n",
    "        vectorstore.add_documents(documents=batch)\n",
    "\n",
    "    return vectorstore"
   ],
   "id": "fe6c7c9f0bc54cee"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Creating the Pinecone vectorstores",
   "id": "7269088c4e86a15"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "rec_vectorstore = create_vectorstore(documents=rec_chunks, index_name='rec-index')\n",
    "sem_vectorstore = create_vectorstore(documents=sem_chunks, index_name='sem-index')\n",
    "sem_g_vectorstore = create_vectorstore(documents=sem_g_chunks, index_name='sem-g-index')"
   ],
   "id": "9d68fe2138be545d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Creating the ground_truth df for each method\n",
    "This dataframe contains the chunks each method created, and the ground_truth references relevant to the question assocuated to the document of the chunk\n"
   ],
   "id": "6cdf1f84b93c9510"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "ground_truth_df = pd.read_excel('evaluation_datasets/ground_truth.xlsx')\n",
    "\n",
    "def create_method_ground_truth_df(chunks, ground_truth_df):\n",
    "    method_df = ground_truth_df.copy()\n",
    "    relevant_chunks_ids = []\n",
    "    relevant_text_indices = set(ground_truth_df['index'])  # Use a set for faster lookups\n",
    "    relevant_chunks = [chunk for chunk in chunks if str(chunk.metadata.get('idx', '')) in map(str, relevant_text_indices)]\n",
    "    print(f\"Total relevant chunks: {len(relevant_chunks)}\")\n",
    "    for i, row in ground_truth_df.iterrows():\n",
    "        idx = row['index']\n",
    "\n",
    "        # Debug print to check values being compared\n",
    "        print(f\"Checking idx: {idx}\")\n",
    "\n",
    "        relevant_chunks_for_row = [\n",
    "            chunk for chunk in relevant_chunks if chunk.metadata['idx'] == idx\n",
    "        ]\n",
    "\n",
    "        # Debug print to see how many relevant chunks were found\n",
    "        print(f\"Relevant chunks for row {i}: {len(relevant_chunks_for_row)}\")\n",
    "\n",
    "        chunks_ids = []\n",
    "        references = row['ground_truth']\n",
    "        for chunk in relevant_chunks_for_row:\n",
    "            # Check if any reference is a substring of chunk.page_content\n",
    "            if any(reference.lower() in chunk.page_content.lower() for reference in references):\n",
    "                chunks_ids.append(chunk.metadata['chunk_index'])\n",
    "        print(f\"Relevant chunks ids: {len(chunks_ids)}\")\n",
    "        relevant_chunks_ids.append(chunks_ids)\n",
    "    method_df['relevant_chunks_ids'] = relevant_chunks_ids\n",
    "    return method_df\n"
   ],
   "id": "e0e4c3c975052bbf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "rec_ground_truth_df = create_method_ground_truth_df(rec_chunks,ground_truth_df)\n",
    "sem_ground_truth_df = create_method_ground_truth_df(sem_chunks,ground_truth_df)\n",
    "sem_g_ground_truth_df = create_method_ground_truth_df(sem_g_chunks,ground_truth_df)"
   ],
   "id": "e68c75375db56b36"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Saving as Parquet file",
   "id": "186e592b9a5644a9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "rec_ground_truth_df.to_parquet(f'{datasets_saving_dir}/rec_ground_truth.parquet', index=False, engine='pyarrow')\n",
    "sem_ground_truth_df.to_parquet(f'{datasets_saving_dir}/sem_ground_truth.parquet', index=False, engine='pyarrow')\n",
    "sem_g_ground_truth_df.to_parquet(f'{datasets_saving_dir}/sem_g_ground_truth.parquet', index=False, engine='pyarrow')"
   ],
   "id": "474f530d53285fcf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# It is possible to start here if the Pinecone vectordbs are set\n",
   "id": "fbee3364034eab06"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import torch\n",
    "\n",
    "# loading the vectorstores again from pinecone\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "embedding_model = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2', model_kwargs={'device': device})\n",
    "\n",
    "index = pc.Index('rec-index')\n",
    "rec_vectorstore = PineconeVectorStore(index=index, embedding=embedding_model)\n",
    "\n",
    "index = pc.Index('sem-index')\n",
    "sem_vectorstore = PineconeVectorStore(index=index, embedding=embedding_model)\n",
    "\n",
    "index = pc.Index('sem-g-index')\n",
    "sem_g_vectorstore = PineconeVectorStore(index=index, embedding=embedding_model)"
   ],
   "id": "280037731a56adce"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Setting up the retrievers, with k=20",
   "id": "9014f47ab1ae21dd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Set retrievers\n",
    "rec_retriever = rec_vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 20}\n",
    ")\n",
    "\n",
    "sem_retriever = sem_vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 20}\n",
    ")\n",
    "\n",
    "sem_g_retriever = sem_g_vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 20}\n",
    ")"
   ],
   "id": "978cd014e324a70"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Loading the Parquet ground-truth files we created for each of the methods",
   "id": "fad7f88b96bcc1e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load ground_truth datasets\n",
    "import pandas as pd\n",
    "\n",
    "rec_ground_truth_df = pd.read_parquet(f'{datasets_saving_dir}/rec_ground_truth.parquet', engine='pyarrow')\n",
    "sem_ground_truth_df = pd.read_parquet(f'{datasets_saving_dir}/sem_ground_truth.parquet', engine='pyarrow')\n",
    "sem_g_ground_truth_df = pd.read_parquet(f'{datasets_saving_dir}/sem_g_ground_truth.parquet', engine='pyarrow')\n"
   ],
   "id": "843f951900221c98"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Compute Recall@k, Precision@k\n",
   "id": "88554f492105e526"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def recall_and_precision_at_k(retriever, ground_truth_df_row, k=5):\n",
    "    # Extract query and ground truth information\n",
    "    query = ground_truth_df_row['question']\n",
    "    query_id = ground_truth_df_row['index']\n",
    "\n",
    "    # Ensure ground_truth_relevant_docs is a list\n",
    "    ground_truth_relevant_docs_idx = list(ground_truth_df_row['relevant_chunks_ids'])\n",
    "\n",
    "    # Retrieve top-K documents\n",
    "    retrieved_at_k = retriever.invoke(query)[:k]\n",
    "\n",
    "    # Filter relevant documents based on metadata\n",
    "    relevant_documents = [\n",
    "        doc for doc in retrieved_at_k\n",
    "        if doc.metadata['idx'] == query_id and doc.metadata['chunk_index'] in ground_truth_relevant_docs_idx\n",
    "    ]\n",
    "\n",
    "    # Handle edge case: no relevant documents in ground truth\n",
    "    if not ground_truth_relevant_docs_idx:\n",
    "        return 0.0  # Avoid division by zero, recall is 0 if no relevant documents exist\n",
    "\n",
    "    # Calculate recall\n",
    "\n",
    "    recall = len(relevant_documents) / len(ground_truth_relevant_docs_idx)\n",
    "    precision = len(relevant_documents) / k\n",
    "    return recall, precision\n",
    "\n",
    "\n",
    "def get_mean_recall_and_precision(retriever, ground_truth_df, k_values):\n",
    "\n",
    "    recall_scores = []\n",
    "    precision_scores = []\n",
    "    for k in k_values:\n",
    "        mean_recall = 0\n",
    "        mean_precision = 0\n",
    "        for _, question_row in ground_truth_df.iterrows():\n",
    "            recall, precision = recall_and_precision_at_k(retriever, question_row, k)\n",
    "            mean_recall += recall\n",
    "            mean_precision += precision\n",
    "        recall_scores.append(mean_recall / len(ground_truth_df))\n",
    "        precision_scores.append(mean_precision / len(ground_truth_df))\n",
    "    return recall_scores, precision_scores"
   ],
   "id": "b322359a8f78277f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Executing the recall and precision calculation function, for each of the chunking methods",
   "id": "d432c1e91ecc572f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "k_values = [1,3,5,10,15,20]\n",
    "\n",
    "rec_recalls, rec_precision = get_mean_recall_and_precision(rec_retriever, rec_ground_truth_df, k_values)\n",
    "sem_recalls, sem_precision = get_mean_recall_and_precision(sem_retriever, sem_ground_truth_df, k_values)\n",
    "sem_g_recalls, sem_g_precision = get_mean_recall_and_precision(sem_g_retriever, sem_g_ground_truth_df, k_values)\n"
   ],
   "id": "68d7ff447a6ee308"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Plotting Recall@k and Precision@k results",
   "id": "6c13028c1e3dba7e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Plotting results for number of chunks\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(k_values, rec_precision, label=\"RecursiveChunker\")\n",
    "plt.plot(k_values, sem_precision, label=\"SemanticChunker\")\n",
    "plt.plot(k_values, sem_g_precision, label=\"SemanticGraphChunker\")\n",
    "\n",
    "# Add labels, title, and legend\n",
    "plt.xlabel(\"K value\")\n",
    "plt.ylabel(\"Mean Precision@k score\")\n",
    "plt.title(\"Comparing Precision@k\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "\n",
    "# Display the plot\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "id": "2d88d8cadc9d316c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Plotting results for number of chunks\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(k_values, rec_recalls, label=\"RecursiveChunker\")\n",
    "plt.plot(k_values, sem_recalls, label=\"SemanticChunker\")\n",
    "plt.plot(k_values, sem_g_recalls, label=\"SemanticGraphChunker\")\n",
    "\n",
    "# Add labels, title, and legend\n",
    "plt.xlabel(\"K value\")\n",
    "plt.ylabel(\"Mean Recall@k score\")\n",
    "plt.title(\"Comparing Recall@k\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "# Display the plot\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "id": "29ea09aa951f3e40"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Generate answers for questions in the dataframes\n",
    "This is the execution of the RAG pipeline, and the settings for the RAGAS evaluation"
   ],
   "id": "945eba92df7ab88a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Defining the prompt for the generator LLM in the pipeline",
   "id": "edac90c44f42878d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "template = \"\"\"\n",
    "You are an intelligent assistant. Answer the query strictly based on the given documents.\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Documents:\n",
    "{documents}\n",
    "\n",
    "Instructions:\n",
    "- Respond **only** based on the provided information.\n",
    "- If the information is insufficient, only respond \"Not enough information to answer.\"\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n"
   ],
   "id": "2a1e721c035f855f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Setting up the LLM from Cohere",
   "id": "a2e59873edcd704a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_community.llms import Cohere\n",
    "\n",
    "llm = Cohere(\n",
    "    model=\"command-xlarge-nightly\",  # Cohere's large model for Q&A with extended context\n",
    "    temperature=0.3,                # Control the randomness of the responses\n",
    "    max_tokens=180,                # Set the maximum response length\n",
    "    cohere_api_key=COHERE_API_KEY\n",
    ")"
   ],
   "id": "954f18a3253c138f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Creating a LangChain chain function to generate response",
   "id": "b8e197d1e066d204"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def generate_answer(query, docs):\n",
    "    from langchain.schema import StrOutputParser\n",
    "    docs_text = [f'Document {i+1}:\\n{doc.page_content}' for i, doc in enumerate(docs)]\n",
    "    documents = '\\n\\n'.join(docs_text)\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    return chain.invoke({'query': query, 'documents': documents})\n"
   ],
   "id": "c9a92661eec10e84"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Creating dataset for RAG evaluation, buy generating answers to the questions, using the RAG pipeline of retriever, then generator",
   "id": "cbf00ba5084f0568"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def creat_evaluation_df(retriever, ground_truth_df):\n",
    "    evaluation_df = ground_truth_df.copy()\n",
    "    generated_answers = []  # Initialize a list to store generated answers\n",
    "    retrieved_documents = []  # Initialize a list to store retrieved documents\n",
    "\n",
    "    # Wrap the loop with tqdm for progress tracking\n",
    "    for _, row in tqdm(ground_truth_df.iterrows(), total=len(ground_truth_df), desc=\"Processing queries\"):\n",
    "        query = row['question']\n",
    "\n",
    "        # Retrieve documents\n",
    "        retrieved_docs = retriever.invoke(query)\n",
    "\n",
    "        # Generate the answer\n",
    "        generated_answer = generate_answer(query=query, docs=retrieved_docs)\n",
    "\n",
    "        # Store results\n",
    "        retrieved_documents.append([doc.page_content for doc in retrieved_docs])\n",
    "        generated_answers.append(generated_answer)\n",
    "\n",
    "    evaluation_df['retrieved_documents'] = retrieved_documents\n",
    "    evaluation_df['generated_answer'] = generated_answers\n",
    "    return evaluation_df\n"
   ],
   "id": "5256e94769860f18"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Executing the function",
   "id": "c1c87356ea36fa58"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "rec_evaluation_df = creat_evaluation_df(rec_retriever, rec_ground_truth_df)\n",
    "sem_evaluation_df = creat_evaluation_df(sem_retriever, sem_ground_truth_df)\n",
    "sem_g_evaluation_df = creat_evaluation_df(sem_g_retriever, sem_g_ground_truth_df)"
   ],
   "id": "99d1363e2a8e3181"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Saving the evaluation datasets as Parquet files",
   "id": "68784553a4a55f87"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "rec_evaluation_df.to_parquet(f'{datasets_saving_dir}/rec_evaluation.parquet', index=False, engine='pyarrow')\n",
    "sem_evaluation_df.to_parquet(f'{datasets_saving_dir}/sem_evaluation.parquet', index=False, engine='pyarrow')\n",
    "sem_g_evaluation_df.to_parquet(f'{datasets_saving_dir}/sem_g_evaluation.parquet', index=False, engine='pyarrow')"
   ],
   "id": "ede1e9554def536c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Loading the evaluation datasets.\n",
    " It is possible to start from here, for the RAGAS results only, if the evaluation datasets are saved"
   ],
   "id": "46332dfe2e5bd280"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "rec_evaluation_df = pd.read_parquet('evaluation_datasets/rec_evaluation.parquet', engine='pyarrow')\n",
    "sem_evaluation_df = pd.read_parquet('evaluation_datasets/sem_evaluation.parquet', engine='pyarrow')\n",
    "sem_g_evaluation_df = pd.read_parquet('evaluation_datasets/sem_g_evaluation.parquet', engine='pyarrow')\n"
   ],
   "id": "9af45ef385131fb2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Evaluate using RAGAS\n",
   "id": "ed3dc9dfcf3954fd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Preprocessing function to prepare the evaluation dataframe for RAGAS requierments",
   "id": "97dc88c374edb094"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "from io import StringIO\n",
    "\n",
    "def preprocess_dataframe(df):\n",
    "    \"\"\"\n",
    "    Preprocess the DataFrame by:\n",
    "    1. Renaming columns.\n",
    "    2. Converting 'retrieved_contexts' from numpy arrays to lists.\n",
    "    3. Cleaning StringIO objects in specified columns.\n",
    "    \"\"\"\n",
    "    # Rename columns\n",
    "    df = df[['question', 'generated_answer', 'retrieved_documents', 'answer']].rename(\n",
    "        columns={\n",
    "            'retrieved_documents': 'retrieved_contexts',\n",
    "            'answer': 'ground_truth',\n",
    "            'generated_answer': 'answer'\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Convert 'retrieved_contexts' from numpy arrays to lists\n",
    "    df['retrieved_contexts'] = df['retrieved_contexts'].apply(\n",
    "        lambda x: x.tolist() if isinstance(x, np.ndarray) else x\n",
    "    )\n",
    "\n",
    "    # Clean StringIO objects in specified columns\n",
    "    columns_to_clean = ['question', 'answer', 'retrieved_contexts', 'ground_truth']\n",
    "    for column in columns_to_clean:\n",
    "        df[column] = df[column].apply(\n",
    "            lambda x: x.getvalue() if isinstance(x, StringIO) else x\n",
    "        )\n",
    "\n",
    "    return df\n"
   ],
   "id": "93fc2aa89920f605"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Apply preprocessing to all DataFrames\n",
    "rec_evaluation_df = preprocess_dataframe(rec_evaluation_df)\n",
    "sem_evaluation_df = preprocess_dataframe(sem_evaluation_df)\n",
    "sem_g_evaluation_df = preprocess_dataframe(sem_g_evaluation_df)"
   ],
   "id": "c942a09409029df2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Setting up the RAGAS evaluator LLM from Cohere",
   "id": "617026552ca1aa22"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_cohere import ChatCohere, CohereEmbeddings\n",
    "\n",
    "# Initialize Cohere models directly\n",
    "llm = ChatCohere(model=\"command-xlarge-nightly\", cohere_api_key=COHERE_API_KEY)\n",
    "embeddings = CohereEmbeddings(model=\"embed-english-v2.0\", cohere_api_key=COHERE_API_KEY)"
   ],
   "id": "b3a03fb304f8f682"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## RAGAS evaluation function",
   "id": "cdeec2155a8e6697"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from ragas.metrics import  Faithfulness, AnswerRelevancy, AnswerCorrectness\n",
    "from ragas import evaluate\n",
    "import pandas as pd\n",
    "import time\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "def ragas_evaluation(evaluation_df, llm, embeddings, k=40, metrics=None):\n",
    "    if metrics is None:\n",
    "        metrics = [\n",
    "            Faithfulness(),\n",
    "            AnswerRelevancy(),\n",
    "            AnswerCorrectness(),\n",
    "        ]\n",
    "    evaluations = []\n",
    "\n",
    "    # Convert DataFrame to Dataset\n",
    "    dataset = Dataset.from_pandas(evaluation_df[:k])\n",
    "\n",
    "    for item in dataset:\n",
    "        # Convert the single item into a pandas DataFrame\n",
    "        single_item_df = pd.DataFrame([item])\n",
    "\n",
    "        # Convert the single-item DataFrame into a Dataset\n",
    "        single_item_dataset = Dataset.from_pandas(single_item_df)\n",
    "\n",
    "        # Evaluate the single item\n",
    "        item_evaluation = evaluate(\n",
    "            metrics=metrics,\n",
    "            dataset=single_item_dataset,\n",
    "            llm=llm,\n",
    "            embeddings=embeddings,\n",
    "        )\n",
    "\n",
    "        # time.sleep(10)\n",
    "\n",
    "        # Convert evaluation result to DataFrame\n",
    "        item_evaluation_df = item_evaluation.to_pandas()\n",
    "        print(item_evaluation_df[['faithfulness', 'answer_relevancy', 'answer_correctness']])\n",
    "        evaluations.append(item_evaluation_df)\n",
    "\n",
    "    # Combine all individual DataFrames into one\n",
    "    return pd.concat(evaluations, ignore_index=True)\n"
   ],
   "id": "710b85ad36d15194"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Creating the Recursive evaluation results data, and saving it",
   "id": "6275cd928bd99b23"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "rec_evaluation_results_df = ragas_evaluation(rec_evaluation_df, llm, embeddings)\n",
    "rec_evaluation_results_df.to_parquet(f'{datasets_saving_dir}/rec_evaluation_results.parquet', index=False, engine='pyarrow')"
   ],
   "id": "15dab03a63dd746c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Creating the Semantic evaluation results data, and saving it\n",
   "id": "584b9717183f735c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "sem_evaluation_results_df = ragas_evaluation(sem_evaluation_df, llm, embeddings)\n",
    "sem_evaluation_results_df.to_parquet(f'{datasets_saving_dir}/sem_evaluation_results.parquet', index=False, engine='pyarrow')\n"
   ],
   "id": "92c72c0fbf614148"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Creating the Semantic-Graph evaluation results data, and saving it\n",
   "id": "ce168d96b78d7bab"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "sem_g_evaluation_results_df = ragas_evaluation(sem_g_evaluation_df, llm, embeddings)\n",
    "sem_g_evaluation_results_df.to_parquet(f'{datasets_saving_dir}/sem_g_evaluation_results.parquet', index=False, engine='pyarrow')\n"
   ],
   "id": "6c2a3fa86dabc3ff"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Visualizing the ragas metrics\n",
   "id": "11b95de2556ffb72"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Loading the evaluation results dataframes",
   "id": "ef0455211edad014"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "rec_evaluation_results_df = pd.read_parquet(f'{datasets_saving_dir}/rec_evaluation_results.parquet', engine='pyarrow')\n",
    "sem_evaluation_results_df = pd.read_parquet(f'{datasets_saving_dir}/sem_evaluation_results.parquet', engine='pyarrow')\n",
    "sem_g_evaluation_results_df = pd.read_parquet(f'{datasets_saving_dir}/sem_g_evaluation_results.parquet', engine='pyarrow')\n"
   ],
   "id": "d463efde616ad50e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def process_and_visualize_from_dataframes(dataframes, column_names, methods):\n",
    "    \"\"\"\n",
    "    Process and visualize metrics from dataframes containing relevant columns.\n",
    "\n",
    "    Parameters:\n",
    "    - dataframes: List of pandas DataFrames (e.g., [rec_df, sem_df, sem_g_df]).\n",
    "    - column_names: List of column names to process (e.g., [\"faithfulness\", \"answer_relevancy\", \"answer_correctness\"]).\n",
    "    - methods: List of method names (e.g., [\"Recursive\", \"Semantic\", \"Semantic Graph\"]).\n",
    "    \"\"\"\n",
    "    for column_name in column_names:\n",
    "        # Combine the columns into a single DataFrame\n",
    "        combined_df = pd.DataFrame({\n",
    "            methods[0]: dataframes[0][column_name],\n",
    "            methods[1]: dataframes[1][column_name],\n",
    "            methods[2]: dataframes[2][column_name]\n",
    "        })\n",
    "\n",
    "        # Drop rows with NaN values in any column\n",
    "        cleaned_df = combined_df.dropna()\n",
    "\n",
    "        # Calculate mean values\n",
    "        mean_values = [cleaned_df[method].mean() for method in methods]\n",
    "\n",
    "        # Create the bar plot\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.bar(methods, mean_values, alpha=0.7, color=[\"blue\", \"green\", \"orange\"])\n",
    "\n",
    "        # Title and labels\n",
    "        metric_title = column_name.replace(\"_\", \" \").title()\n",
    "        plt.title(f\"Average {metric_title} Across Methods\", fontsize=14)\n",
    "        plt.ylabel(f\"Average {metric_title}\", fontsize=12)\n",
    "        plt.xlabel(\"Methods\", fontsize=12)\n",
    "        plt.grid(alpha=0.5, linestyle='--', axis='y')\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Show the plot\n",
    "        plt.show()\n",
    "\n",
    "# Specify the column names and method names\n",
    "column_names = [\"faithfulness\", \"answer_relevancy\", \"answer_correctness\"]\n",
    "methods = [\"Recursive\", \"Semantic\", \"Semantic Graph\"]\n",
    "\n",
    "# Call the function with the loaded data\n",
    "process_and_visualize_from_dataframes(\n",
    "    dataframes=[rec_evaluation_results_df, sem_evaluation_results_df, sem_g_evaluation_results_df],\n",
    "    column_names=column_names,\n",
    "    methods=methods\n",
    ")\n"
   ],
   "id": "568530b186619a12"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
